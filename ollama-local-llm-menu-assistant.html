<!DOCTYPE html>
<html lang="tr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Ollama ve Open WebUI kullanarak 100+ menÃ¼ Ã¶ÄŸesi arasÄ±nda yÃ¶nlendirme yapan LLM asistanÄ± geliÅŸtirme deneyimim.">
    <title>Ollama ile Local LLM: Kurumsal MenÃ¼ AsistanÄ± GeliÅŸtirme | Remzi Åahbaz Blog</title>
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;700&family=Crimson+Pro:wght@400;600;700&display=swap" rel="stylesheet">
    
    <link rel="stylesheet" href="../styles.css">
    <link rel="stylesheet" href="post-styles.css">
</head>
<body>
    <nav class="nav">
        <div class="nav-container">
            <a href="../index.html" class="logo">
                <span class="logo-bracket">&lt;</span>
                <span class="logo-text">Remzi</span>
                <span class="logo-bracket">/&gt;</span>
            </a>
            <div class="nav-links">
                <a href="../index.html#articles" class="nav-link">Makaleler</a>
                <a href="../index.html#about" class="nav-link">HakkÄ±mda</a>
                <a href="https://github.com/remzisahbaz" target="_blank" class="nav-link">GitHub</a>
            </div>
        </div>
    </nav>

    <article class="post">
        <header class="post-header">
            <div class="post-meta-top">
                <a href="../index.html" class="back-link">â† Geri</a>
                <span class="post-tag">AI / LLM</span>
            </div>
            <h1 class="post-title">Ollama ile Local LLM: Kurumsal MenÃ¼ AsistanÄ± GeliÅŸtirme</h1>
            <div class="post-meta">
                <span class="meta-item">Remzi Åahbaz</span>
                <span class="meta-separator">â€¢</span>
                <span class="meta-item">17 Åubat 2026</span>
                <span class="meta-separator">â€¢</span>
                <span class="meta-item">15 dk okuma</span>
            </div>
        </header>

        <div class="post-content">
            <p class="post-lead">
                Kurumsal bir web uygulamasÄ±nda 100+ menÃ¼ Ã¶ÄŸesi arasÄ±nda kullanÄ±cÄ±larÄ± doÄŸru sayfaya 
                yÃ¶nlendiren bir LLM asistanÄ± geliÅŸtirdim. Cloud LLM yerine local Ollama altyapÄ±sÄ± 
                kullanarak hem maliyeti sÄ±fÄ±rladÄ±m hem de veri gÃ¼venliÄŸini saÄŸladÄ±m. Bu yazÄ±da, 
                4 farklÄ± modeli test etme sÃ¼recimi ve Mistral Small 24B ile %100 doÄŸruluk elde 
                etme hikayemi paylaÅŸÄ±yorum.
            </p>

            <h2>Problem: 100+ MenÃ¼ Ä°Ã§inde Kaybolmak</h2>
            <p>
                GeliÅŸtirdiÄŸimiz kurumsal ERP sisteminde 100'den fazla menÃ¼ Ã¶ÄŸesi vardÄ±. KullanÄ±cÄ±lar 
                sÄ±k sÄ±k "beyanname girmek istiyorum" veya "rapor gÃ¶rmek istiyorum" gibi sorunlarla 
                destek ekibine geliyordu. Klasik arama Ã§Ã¶zÃ¼mleri yetersiz kalÄ±yordu Ã§Ã¼nkÃ¼:
            </p>

            <ul>
                <li><strong>Nested (iÃ§ iÃ§e) menÃ¼ler</strong>: 3-4 seviye derinlikte alt menÃ¼ler</li>
                <li><strong>Benzer isimler</strong>: "Rapor" kelimesi 12 farklÄ± menÃ¼de geÃ§iyordu</li>
                <li><strong>TÃ¼rkÃ§e keyword matching</strong>: "gb" â†’ "GÃ¼mrÃ¼k Beyannamesi" eÅŸleÅŸmesi zordu</li>
                <li><strong>Rol bazlÄ± eriÅŸim</strong>: Her kullanÄ±cÄ± kendi yetkili olduÄŸu menÃ¼leri gÃ¶rmeli</li>
            </ul>

            <h2>Neden Local LLM?</h2>

            <h3>Cloud LLM'in DezavantajlarÄ±:</h3>
            <ul>
                <li><strong>Maliyet</strong>: API call baÅŸÄ±na Ã¼cret (ayda binlerce sorgu = yÃ¼ksek fatura)</li>
                <li><strong>Veri gÃ¼venliÄŸi</strong>: MenÃ¼ yapÄ±sÄ± ve kullanÄ±cÄ± bilgileri 3rd party'ye gidiyor</li>
                <li><strong>Latency</strong>: Her istek internet Ã¼zerinden gidip geliyor</li>
                <li><strong>BaÄŸÄ±mlÄ±lÄ±k</strong>: API down olursa sistem Ã§alÄ±ÅŸmÄ±yor</li>
            </ul>

            <h3>Local LLM'in AvantajlarÄ±:</h3>
            <ul>
                <li>âœ… <strong>$0 maliyet</strong>: Sadece GPU elektriÄŸi</li>
                <li>âœ… <strong>Tam veri kontrolÃ¼</strong>: HiÃ§bir ÅŸey dÄ±ÅŸarÄ± Ã§Ä±kmÄ±yor</li>
                <li>âœ… <strong>DÃ¼ÅŸÃ¼k latency</strong>: Local network Ã¼zerinden ~200ms</li>
                <li>âœ… <strong>Offline Ã§alÄ±ÅŸma</strong>: Ä°nternet baÄŸÄ±mlÄ±lÄ±ÄŸÄ± yok</li>
            </ul>

            <h2>Teknoloji Stack</h2>

            <div class="code-block">
                <div class="code-header">
                    <span class="code-lang">yaml</span>
                    <span class="code-title">Tech Stack</span>
                </div>
                <pre><code># Local LLM Infrastructure
Platform: Ollama v0.1.22
Frontend: Open WebUI v0.2.5
GPU: NVIDIA RTX 4090 (24GB VRAM)

# Models Tested
- Mistral Small 24B (WINNER)
- Gemma 3 12B
- Gemma 3 4B
- Llama 3.1 8B
- Qwen3-Coder 14B
- DeepSeek R1 14B

# Backend Integration
Spring Boot 3.2
RestTemplate for API calls
JSON parsing with Jackson</code></pre>
            </div>

            <h2>Ollama Kurulumu</h2>

            <h3>1. Ollama Installation (Ubuntu/Linux)</h3>
            <div class="code-block">
                <div class="code-header">
                    <span class="code-lang">bash</span>
                    <span class="code-title">Ollama Kurulumu</span>
                </div>
                <pre><code># Ollama install
curl -fsSL https://ollama.com/install.sh | sh

# GPU driver check
nvidia-smi

# Model download (Mistral Small 24B - ~14GB)
ollama pull mistral-small:24b

# Test run
ollama run mistral-small:24b
>>> Merhaba!
Merhaba! Size nasÄ±l yardÄ±mcÄ± olabilirim?</code></pre>
            </div>

            <h3>2. Open WebUI Kurulumu (Docker)</h3>
            <div class="code-block">
                <div class="code-header">
                    <span class="code-lang">bash</span>
                    <span class="code-title">Open WebUI Docker</span>
                </div>
                <pre><code># Docker ile Open WebUI
docker run -d -p 3000:8080 \
  --gpus all \
  --add-host=host.docker.internal:host-gateway \
  -v open-webui:/app/backend/data \
  --name open-webui \
  --restart always \
  ghcr.io/open-webui/open-webui:main

# UI'ye eriÅŸim
# http://localhost:3000</code></pre>
            </div>

            <h2>System Prompt Engineering</h2>

            <p>
                LLM'in doÄŸru Ã§alÄ±ÅŸmasÄ± iÃ§in kritik olan kÄ±sÄ±m: <strong>System Prompt</strong>. 
                Ä°ÅŸte 20+ iterasyon sonrasÄ± optimize ettiÄŸim prompt:
            </p>

            <div class="code-block">
                <div class="code-header">
                    <span class="code-lang">plaintext</span>
                    <span class="code-title">System Prompt (Final Version)</span>
                </div>
                <pre><code>Sen bir web uygulamasÄ± menÃ¼ yÃ¶nlendirme asistanÄ±sÄ±n.

## KRÄ°TÄ°K KURALLAR:
1. SADECE menÃ¼ yÃ¶nlendirmesi yap
2. MenÃ¼ dÄ±ÅŸÄ± sorulara: "ÃœzgÃ¼nÃ¼m, ben sadece menÃ¼ yÃ¶nlendirmesi yapabilirim"
3. MUTLAKA JSON formatÄ±nda yanÄ±t ver
4. Nested (iÃ§ iÃ§e) menÃ¼leri recursive ara
5. Belirsiz durumlarda TÃœM eÅŸleÅŸmeleri dÃ¶ndÃ¼r

## MENÃœ YAPISI:
100+ menÃ¼ Ã¶ÄŸesi, 3-4 seviye derinlik
Ã–rnek: Ana MenÃ¼ â†’ Beyanname â†’ GÃ¼mrÃ¼k Beyannnamesi â†’ GB Ä°zleme

## YANIT FORMATI:
{
  "matches": [
    {
      "path": "Sol menÃ¼ â†’ Beyanname â†’ GB Ä°zleme",
      "route": "/declaration/list?type=GB",
      "permission": "READ_DECLARATION"
    }
  ],
  "total": 1,
  "query": "kullanÄ±cÄ±nÄ±n sorusu"
}

## Ã–RNEK SORGULAR:
"rapor gÃ¶rmek istiyorum" â†’ 12 farklÄ± rapor menÃ¼sÃ¼nÃ¼ dÃ¶ndÃ¼r
"beyanname girmek istiyorum" â†’ Beyanname GiriÅŸ sayfasÄ±
"gb izlemek istiyorum" â†’ GB Ä°zleme sayfasÄ±</code></pre>
            </div>

            <h2>Model KarÅŸÄ±laÅŸtÄ±rmasÄ±: 4 FarklÄ± Test</h2>

            <p>
                "rapor" kelimesini arayarak her modeli test ettim. Sistem menÃ¼lerinde 
                12 farklÄ± rapor sayfasÄ± var. Ä°deal sonuÃ§: 12/12 eÅŸleÅŸme.
            </p>

            <h3>Test SonuÃ§larÄ±:</h3>

            <div class="stats-grid">
                <div class="stat-card">
                    <div class="stat-number">1/12</div>
                    <div class="stat-label">Gemma 3 4B</div>
                </div>
                <div class="stat-card">
                    <div class="stat-number">5/12</div>
                    <div class="stat-label">Llama 3.1 8B</div>
                </div>
                <div class="stat-card">
                    <div class="stat-number">6/12</div>
                    <div class="stat-label">Gemma 3 12B</div>
                </div>
                <div class="stat-card success">
                    <div class="stat-number">12/12</div>
                    <div class="stat-label">Mistral Small 24B âœ…</div>
                </div>
            </div>

            <h3>DetaylÄ± Analiz:</h3>

            <h4>âŒ Gemma 3 4B (1/12 - En KÃ¶tÃ¼)</h4>
            <ul>
                <li>Sadece 1 menÃ¼ buldu, 11 tanesini kaÃ§Ä±rdÄ±</li>
                <li>JSON formatÄ±nÄ± bozdu, path_description'a prompt metnini kopyaladÄ±</li>
                <li>Nested arama yapamÄ±yor</li>
                <li>4B parametre kÃ¼Ã§Ã¼k gÃ¶revler iÃ§in bile yetersiz</li>
            </ul>

            <h4>ğŸŸ¡ Llama 3.1 8B (5/12 - KÃ¶tÃ¼)</h4>
            <ul>
                <li>5 menÃ¼ buldu ama 7 tanesini kaÃ§Ä±rdÄ±</li>
                <li>Kural uyumu zayÄ±f</li>
                <li>Format doÄŸruluÄŸu bozuk</li>
                <li>8B bile bu gÃ¶rev iÃ§in yetersiz</li>
            </ul>

            <h4>ğŸŸ¡ Gemma 3 12B (6/12 - Orta)</h4>
            <ul>
                <li>YarÄ±sÄ±nÄ± buldu, yarÄ±sÄ±nÄ± kaÃ§Ä±rdÄ±</li>
                <li>JSON formatÄ± iyi</li>
                <li>Nested aramayÄ± kÄ±smen yapÄ±yor</li>
                <li>Production iÃ§in gÃ¼venilir deÄŸil</li>
            </ul>

            <h4>âœ… Mistral Small 24B (12/12 - PERFECT)</h4>
            <ul>
                <li>TÃ¼m 12 rapor menÃ¼sÃ¼nÃ¼ buldu</li>
                <li>MÃ¼kemmel JSON formatÄ±</li>
                <li>Recursive nested arama yapÄ±yor</li>
                <li>Instruction-following Ã§ok iyi</li>
                <li>Production-ready</li>
            </ul>

            <h2>Neden Mistral Small 24B KazandÄ±?</h2>

            <h3>1. Model Parametresi Ã–nemli</h3>
            <p>
                24B parametre, complex instruction'larÄ± anlama ve JSON gibi yapÄ±landÄ±rÄ±lmÄ±ÅŸ 
                output Ã¼retmede kritik fark yaratÄ±yor. 4B/8B modeller basit task'ler iÃ§in 
                yeterli ama kurumsal seviye ihtiyaÃ§lar iÃ§in yetersiz kalÄ±yor.
            </p>

            <h3>2. Instruction-Following Kalitesi</h3>
            <p>
                Mistral Small, system prompt'taki kurallarÄ± harfiyen takip etti. DiÄŸer modeller 
                sÄ±k sÄ±k prompt'tan sapma gÃ¶sterdi.
            </p>

            <h3>3. Nested Search Capability</h3>
            <p>
                3-4 seviye derinlikteki menÃ¼leri recursive olarak tarayabilme yeteneÄŸi sadece 
                Mistral Small'da var. DiÄŸerleri yÃ¼zeysel arama yapÄ±yor.
            </p>

            <h2>Spring Boot Entegrasyonu</h2>

            <div class="code-block">
                <div class="code-header">
                    <span class="code-lang">java</span>
                    <span class="code-title">OllamaService.java</span>
                </div>
                <pre><code>@Service
@Slf4j
public class OllamaMenuAssistantService {

    private static final String OLLAMA_API_URL = "http://localhost:11434/api/generate";
    private static final String MODEL_NAME = "mistral-small:24b";
    
    @Autowired
    private RestTemplate restTemplate;
    
    @Autowired
    private ObjectMapper objectMapper;

    public MenuAssistantResponse findMenu(String userQuery) {
        try {
            // Ollama request payload
            Map<String, Object> request = new HashMap<>();
            request.put("model", MODEL_NAME);
            request.put("prompt", userQuery);
            request.put("system", getSystemPrompt());
            request.put("stream", false);
            request.put("format", "json");

            // API call
            log.info("Ollama API call: {}", userQuery);
            long startTime = System.currentTimeMillis();
            
            ResponseEntity<String> response = restTemplate.postForEntity(
                OLLAMA_API_URL,
                request,
                String.class
            );
            
            long duration = System.currentTimeMillis() - startTime;
            log.info("Ollama response time: {}ms", duration);

            // Parse JSON response
            JsonNode root = objectMapper.readTree(response.getBody());
            String llmOutput = root.get("response").asText();
            
            return objectMapper.readValue(llmOutput, MenuAssistantResponse.class);
            
        } catch (Exception e) {
            log.error("Ollama API error", e);
            return MenuAssistantResponse.error("MenÃ¼ arama hatasÄ±");
        }
    }

    private String getSystemPrompt() {
        // System prompt buraya (yukarÄ±da paylaÅŸtÄ±m)
        return "Sen bir web uygulamasÄ± menÃ¼ yÃ¶nlendirme asistanÄ±sÄ±n...";
    }
}</code></pre>
            </div>

            <h2>Performance Metrics</h2>

            <div class="stats-grid">
                <div class="stat-card highlight">
                    <div class="stat-number">~200ms</div>
                    <div class="stat-label">Avg Response Time</div>
                </div>
                <div class="stat-card success">
                    <div class="stat-number">100%</div>
                    <div class="stat-label">Accuracy (12/12)</div>
                </div>
                <div class="stat-card highlight">
                    <div class="stat-number">$0</div>
                    <div class="stat-label">Monthly Cost</div>
                </div>
            </div>

            <h3>Maliyet KarÅŸÄ±laÅŸtÄ±rmasÄ±: Cloud vs Local</h3>

            <div class="code-block">
                <div class="code-header">
                    <span class="code-lang">plaintext</span>
                    <span class="code-title">Cost Analysis (Monthly)</span>
                </div>
                <pre><code># Cloud LLM (OpenAI GPT-4)
10,000 queries/month Ã— $0.03/1K tokens = ~$300/month
+ Latency: 500-1000ms

# Local LLM (Mistral Small 24B)
Electricity: ~$20/month (GPU power)
+ Latency: 200ms
= $280/month savings ğŸ’°</code></pre>
            </div>

            <h2>Zorluklar ve Ã‡Ã¶zÃ¼mler</h2>

            <h3>1. VRAM Limiti</h3>
            <p>
                <strong>Problem</strong>: Mistral Small 24B, full precision'da ~48GB VRAM istiyor. 
                RTX 4090'Ä±m sadece 24GB.
            </p>
            <p>
                <strong>Ã‡Ã¶zÃ¼m</strong>: 4-bit quantization (Q4_K_M) kullandÄ±m. Model boyutu 48GB'den 
                14GB'ye dÃ¼ÅŸtÃ¼, accuracy kaybÄ± minimal (~2%).
            </p>

            <div class="code-block">
                <div class="code-header">
                    <span class="code-lang">bash</span>
                    <span class="code-title">Quantized Model</span>
                </div>
                <pre><code># Quantized model pull
ollama pull mistral-small:24b-q4_K_M

# VRAM usage check
nvidia-smi
# 14GB / 24GB (safe margin)</code></pre>
            </div>

            <h3>2. Prompt Drift (Kural Sapmasi)</h3>
            <p>
                <strong>Problem</strong>: LLM bazen system prompt'taki kurallarÄ± unutuyor, 
                menÃ¼ dÄ±ÅŸÄ± sorulara cevap vermeye Ã§alÄ±ÅŸÄ±yor.
            </p>
            <p>
                <strong>Ã‡Ã¶zÃ¼m</strong>: Prompt'ta "ASLA", "MUTLAKA", "SADECE" gibi gÃ¼Ã§lÃ¼ 
                keyword'ler kullandÄ±m. AyrÄ±ca her istek baÅŸÄ±nda kurallarÄ± tekrarladÄ±m.
            </p>

            <h3>3. JSON Parse HatasÄ±</h3>
            <p>
                <strong>Problem</strong>: Bazen LLM valid JSON yerine Markdown wrapped JSON dÃ¶ndÃ¼rÃ¼yordu:
            </p>

            <div class="code-block">
                <div class="code-header">
                    <span class="code-lang">json</span>
                    <span class="code-title">Invalid Output Example</span>
                </div>
                <pre><code>```json
{
  "matches": [...]
}
```</code></pre>
            </div>

            <p>
                <strong>Ã‡Ã¶zÃ¼m</strong>: Backend'de regex ile Markdown fence'leri temizledim:
            </p>

            <div class="code-block">
                <div class="code-header">
                    <span class="code-lang">java</span>
                    <span class="code-title">JSON Cleanup</span>
                </div>
                <pre><code>String cleanJson = llmOutput
    .replaceAll("```json\\n?", "")
    .replaceAll("```\\n?", "")
    .trim();

return objectMapper.readValue(cleanJson, MenuAssistantResponse.class);</code></pre>
            </div>

            <h2>Production Deployment</h2>

            <h3>Docker Compose Setup</h3>

            <div class="code-block">
                <div class="code-header">
                    <span class="code-lang">yaml</span>
                    <span class="code-title">docker-compose.yml</span>
                </div>
                <pre><code>version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    volumes:
      - open-webui-data:/app/backend/data
    depends_on:
      - ollama
    restart: unless-stopped

volumes:
  ollama-data:
  open-webui-data:</code></pre>
            </div>

            <h2>Ã–ÄŸrendiklerim ve Tavsiyeler</h2>

            <div class="tips-box">
                <h3>âœ… YapÄ±lmasÄ± Gerekenler:</h3>
                <ul>
                    <li><strong>Model seÃ§imi kritik</strong>: 24B+ parametre production iÃ§in minimum</li>
                    <li><strong>Quantization kullan</strong>: VRAM'i optimize et, accuracy kaybÄ± minimal</li>
                    <li><strong>Prompt engineering'e zaman ayÄ±r</strong>: 20+ iterasyon normal</li>
                    <li><strong>Fallback mekanizmasÄ±</strong>: LLM fail ederse klasik search devreye girsin</li>
                    <li><strong>Monitoring ekle</strong>: Response time, error rate, accuracy tracking</li>
                    <li><strong>A/B testing yap</strong>: FarklÄ± modelleri gerÃ§ek kullanÄ±cÄ±larla test et</li>
                </ul>
            </div>

            <div class="warning-box">
                <h3>âŒ KaÃ§Ä±nÄ±lmasÄ± Gerekenler:</h3>
                <ul>
                    <li><strong>KÃ¼Ã§Ã¼k modellere gÃ¼venme</strong>: 4B/8B production iÃ§in risk</li>
                    <li><strong>Prompt'u deÄŸiÅŸtirmeden canlÄ±ya alma</strong>: Mutlaka test et</li>
                    <li><strong>VRAM limitini aÅŸma</strong>: OOM (Out of Memory) crash'e sebep olur</li>
                    <li><strong>Error handling ihmal etme</strong>: LLM bazen beklenmedik output verir</li>
                    <li><strong>Cloud LLM'e kÃ¶rÃ¼ kÃ¶rÃ¼ne geÃ§me</strong>: Local alternatif genelde daha iyi</li>
                </ul>
            </div>

            <h2>Gelecek PlanlarÄ±</h2>

            <h3>1. RAG (Retrieval Augmented Generation) Entegrasyonu</h3>
            <p>
                MenÃ¼ yapÄ±sÄ±nÄ± vector database'e (Chroma/Qdrant) indexleyerek semantic search 
                ekleyeceÄŸim. BÃ¶ylece "fatura" â†’ "invoice" gibi cross-language query'ler de Ã§alÄ±ÅŸacak.
            </p>

            <h3>2. Fine-tuning</h3>
            <p>
                KullanÄ±cÄ± sorgu geÃ§miÅŸinden dataset oluÅŸturup model'i fine-tune edeceÄŸim. 
                Domain-specific performansÄ± daha da artÄ±racak.
            </p>

            <h3>3. Multi-modal Support</h3>
            <p>
                Screenshot'tan menÃ¼ bulma: KullanÄ±cÄ± ekran gÃ¶rÃ¼ntÃ¼sÃ¼ atÄ±nca LLM o sayfaya giden 
                yolu gÃ¶sterecek.
            </p>

            <h2>SonuÃ§</h2>

            <p>
                Local LLM ile kurumsal bir problemi Ã§Ã¶zdÃ¼m: 100+ menÃ¼ arasÄ±nda yÃ¶nlendirme. 
                Cloud LLM yerine Ollama + Mistral Small 24B kullanarak:
            </p>

            <ul>
                <li>âœ… $280/ay maliyet tasarrufu</li>
                <li>âœ… %100 veri gÃ¼venliÄŸi (hiÃ§bir veri dÄ±ÅŸarÄ± Ã§Ä±kmÄ±yor)</li>
                <li>âœ… 200ms latency (cloud'dan 3-5x hÄ±zlÄ±)</li>
                <li>âœ… 12/12 accuracy (production-ready)</li>
            </ul>

            <p>
                En Ã¶nemli ders: Model parametresi kritik. 4B/8B modeller hobi projeleri iÃ§in 
                yeterli ama kurumsal ihtiyaÃ§lar iÃ§in 24B+ gerekli. Prompt engineering ve 
                quantization ile local LLM'ler artÄ±k production-ready seviyede.
            </p>

            <div class="cta-box">
                <h3>SorularÄ±nÄ±z mÄ± var?</h3>
                <p>
                    Local LLM, Ollama veya prompt engineering hakkÄ±nda soru ve deneyimlerinizi 
                    <a href="mailto:sahbazremzii@gmail.com">email</a> veya 
                    <a href="https://linkedin.com/in/remzisahbaz090" target="_blank">LinkedIn</a> 
                    Ã¼zerinden benimle paylaÅŸabilirsiniz.
                </p>
            </div>

            <div class="post-tags">
                <span class="tag-label">Etiketler:</span>
                <a href="#" class="tag">LLM</a>
                <a href="#" class="tag">Ollama</a>
                <a href="#" class="tag">Mistral</a>
                <a href="#" class="tag">AI</a>
                <a href="#" class="tag">Prompt Engineering</a>
                <a href="#" class="tag">Local AI</a>
            </div>
        </div>

        <footer class="post-footer">
            <div class="author-card">
                <div class="author-info">
                    <h3>Remzi Åahbaz</h3>
                    <p>Software Engineer @ ExperiLabs</p>
                    <p class="author-bio">
                        4+ yÄ±ldÄ±r banking ve fintech sektÃ¶rÃ¼nde Spring Boot, mikroservisler ve 
                        AI/LLM teknolojileriyle Ã§alÄ±ÅŸÄ±yorum.
                    </p>
                </div>
                <div class="author-links">
                    <a href="https://github.com/remzisahbaz" target="_blank">GitHub</a>
                    <a href="https://linkedin.com/in/remzisahbaz090" target="_blank">LinkedIn</a>
                </div>
            </div>
        </footer>
    </article>

    <footer class="footer">
        <div class="footer-content">
            <div class="footer-left">
                <div class="footer-logo">
                    <span class="logo-bracket">&lt;</span>
                    <span class="logo-text">Remzi</span>
                    <span class="logo-bracket">/&gt;</span>
                </div>
                <p class="footer-tagline">Kod, Mimari ve Mikroservisler</p>
            </div>
            <div class="footer-right">
                <p>&copy; 2026 Remzi Åahbaz. TÃ¼m haklarÄ± saklÄ±dÄ±r.</p>
            </div>
        </div>
    </footer>

    <script src="../script.js"></script>
</body>
</html>
